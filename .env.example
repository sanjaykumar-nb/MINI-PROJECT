# Enhanced RAG System v7.0 - Environment Configuration
# ======================================================
# Copy this file to .env and fill in your API keys

# ============================================================================
# REQUIRED API KEYS (Recommended for best performance)
# ============================================================================

# Groq API - Fast cloud inference (Free tier: 30 requests/min)
# Get your key at: https://console.groq.com/
GROQ_API_KEY=your_groq_api_key_here

# HuggingFace Token - Fallback models (Free)
# Get your token at: https://huggingface.co/settings/tokens
HF_TOKEN=your_huggingface_token_here

# ============================================================================
# OPTIONAL API KEYS (For enhanced research features)
# ============================================================================

# PubMed API - Medical research papers (Optional, increases rate limits)
# Get key at: https://www.ncbi.nlm.nih.gov/account/
PUBMED_API_KEY=

# arXiv API - Academia papers (No key needed, but can add if you have one)
ARXIV_API_KEY=

# CrossRef API - Academic citations (Optional)
CROSSREF_API_KEY=

# OpenAIRE API - European research (Optional)
OPENAIRE_API_KEY=

# ============================================================================
# FEATURE TOGGLES (true/false)
# ============================================================================

# Enable remote API calls (Groq, HuggingFace, research APIs)
# Set to false for local-only mode (uses only Ollama)
ENABLE_REMOTE_APIS=true

# Enable clipboard extraction feature
# Requires pyperclip: pip install pyperclip
ENABLE_CLIPBOARD=true

# Enable deep research mode (multi-API paper search)
# Requires ENABLE_REMOTE_APIS=true
ENABLE_DEEP_RESEARCH=true

# ============================================================================
# LOCAL AI SETTINGS
# ============================================================================

# Ollama base URL (local AI - prioritized for privacy and speed)
# Default: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# Ollama model to use
# Default: llama3.2:3b (small, fast)
# Other options: llama3:8b, mistral:7b, phi3:mini
OLLAMA_MODEL=llama3.2:3b

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================

# Daily token limit (to control API costs)
DAILY_TOKEN_LIMIT=50000

# Cache duration in hours
CACHE_DURATION_HOURS=24

# Maximum chunks to use for context
MAX_CHUNKS_FOR_CONTEXT=8

# Chunk size for document processing
CHUNK_SIZE=800

# Chunk overlap
CHUNK_OVERLAP=200

# ============================================================================
# CLIPBOARD SETTINGS
# ============================================================================

# Maximum clipboard content length (characters)
CLIPBOARD_MAX_LENGTH=50000

# Auto-process clipboard changes (monitors clipboard)
# Warning: Can be resource intensive
CLIPBOARD_AUTO_PROCESS=false

# ============================================================================
# DEEP RESEARCH SETTINGS
# ============================================================================

# Maximum research papers to fetch
DEEP_RESEARCH_MAX_PAPERS=20

# Maximum tokens for deep research analysis
DEEP_RESEARCH_MAX_TOKENS=2000

# ============================================================================
# NOTES
# ============================================================================

# 1. Keep your .env file private - never commit it to version control
# 2. The system works without API keys, but with limited features
# 3. Ollama runs locally and doesn't need API keys
# 4. All settings have sensible defaults if not specified
# 5. Boolean values: true, false, 1, 0, yes, no, on, off

# ============================================================================
# QUICK SETUP GUIDE
# ============================================================================

# 1. Copy this file: cp .env.example .env
# 2. Edit .env and add your API keys
# 3. Save the file
# 4. Run the system: python v7_complete.py

# For local-only mode (no API keys needed):
# - Install Ollama: https://ollama.ai/
# - Pull a model: ollama pull llama3.2:3b
# - Set ENABLE_REMOTE_APIS=false
# - Run the system!
